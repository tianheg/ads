# 入门

## 为什么学

- 通过大厂面试
- 实际工作中用得到
- 自我价值实现
- 学好能更方便阅读框架源码，理解背后的设计思想
- 提升代码性能

## 重点何处

从广义上讲，数据结构就是指一组数据的存储结构。算法就是操作数据的一组方法。从狭义上讲，是指某些著名的数据结构和算法，比如队列、栈、堆、二分查找、动态规划等。**数据结构是为算法服务的，算法要作用在特定的数据结构之上**。

重点在**复杂度分析**。

最常用、最基础的 20 个数据结构和算法：

- 10 个数据结构：**数组、链表、栈、队列、散列表、二叉树、堆、跳表、图、Trie 树**；
- 10 个算法：**递归、排序、二分查找、搜索、哈希算法、贪心算法、分治算法、回溯算法、动态规划、字符串匹配算法**。

学习它们的“来历”“自身的特点”“适合解决的问题”以及“实际的应用场景”。

## 学习技巧

1. 边学边练，适度刷题
2. 多问、多思考、多互动
3. 目标学习法
4. 沉淀知识
5. 当我们要谈一个事物/概念的时候，需要问自己三个终极问题——是什么？为什么？怎么样？

## 复杂度分析

复杂度分析分为时间和空间。

事后统计法【通过设计好的测试程序和数据，利用计算机计时器对不同算法编制的程序的运行时间进行比较，从而确定算法效率的高低】的局限性：

1. 测试结果非常依赖测试环境
2. 测试结果受数据规模的影响很大

### 大 O 复杂度表示法

它只表示一种变化趋势，且忽略（公式中的）常量、低阶、系数，只需要记录一个最大阶的量级就可以了。

```c
int cal(int n) {
    int sum = 0;
    int i = 1;
    for (; i <= n; ++i) {
        sum = sum + i;
    }
    return sum;
}
```

这段代码的每一行都进行“读数据-运算-写数据”。尽管每行代码对应的 CPU 执行的个数、执行的时间都不一样，但是，我们这里只是粗略估计，所以可以假设每行代码执行的时间都一样，为 unit_time。

第 2、3 行代码分别需要 1 个 unit_time 的执行时间，第 4、5 行都运行了 n 遍，所以需要 `2n*unit_time` 的执行时间，所以**这段代码总的执行时间**就是 `(2n+2)*unit_time`。所有代码的执行时间 T(n) 与每行代码的执行次数成正比。

```c
int cal(int n) {
    int sum = 0;
    int i = 1;
    int j = 1;
    for (; i <= n; ++i) {
        j = 1;
        for (; j <= n; ++j) {
            sum = sum +  i * j;
        }
    }
}
```

第 2、3、4 行代码，每行都需要 1 个 unit_time 的执行时间，第 5、6 行代码循环执行了 n 遍，需要 2n * unit_time 的执行时间，第 7、8 行代码循环执行了 n2遍，所以需要 `2n^2 * unit_time` 的执行时间。所以，整段代码总的执行时间 `T(n) = (2n^2+2n+3)*unit_time`。

$$T(n)=Of(n)$$

- $T(n)$：代码执行的时间
- $n$：数据规模的大小
- $f(n)$：每行代码执行的次数总和
- $O$：代码的执行时间 $T(n)$ 与 $f(n)$ 表达式成正比

故例子一中的 $T(n)=O(2n+2)$，例子二中的 $T(n)=O(2n^{2}+2n+3)$。这就是 **大 O 时间复杂度表示法**。它不代表具体代码的执行时间，而是表示 **代码执行时间随数据规模增长的变化趋势**，所以，也叫作 **渐进时间复杂度**(asymptotic time complexity)，简称 **时间复杂度**。

**当 n 很大时，你可以把它想象成 10000、100000。而公式中的低阶、常量、系数三部分并不左右增长趋势，所以都可以忽略**。我们只需要记录一个最大量级就可以了，如果用大 O 表示法表示刚讲的那两段代码的时间复杂度，就可以记为：$T(n) = O(n)$； $T(n) = O(n^{2})$。

### 时间复杂度分析

**1. 只关注循环执行次数最多的一段代码**

在分析一个算法、一段代码的时间复杂度的时候，也只关注循环执行次数最多的那一段代码就可以了。

```c
int cal(int n) {
    int sum = 0;
    int i = 1;
    for (; i <= n; ++i) {
        sum = sum + i;
    }
    return sum;
}
```

其中第 2、3 行代码都是常量级的执行时间，与 n 的大小无关，所以对于复杂度并没有影响。循环执行次数最多的是第 4、5 行代码，所以这块代码要重点分析。前面我们也讲过，这两行代码被执行了 n 次，所以总的时间复杂度就是 O(n)。

**2. 加法法则：总复杂度等于量级最大的那段代码的复杂度**

```c
int cal(int n) {
   int sum_1 = 0;
   int p = 1;
   for (; p < 100; ++p) {
     sum_1 = sum_1 + p;
   }

   int sum_2 = 0;
   int q = 1;
   for (; q < n; ++q) {
     sum_2 = sum_2 + q;
   }

   int sum_3 = 0;
   int i = 1;
   int j = 1;
   for (; i <= n; ++i) {
     j = 1;
     for (; j <= n; ++j) {
       sum_3 = sum_3 +  i * j;
     }
   }

   return sum_1 + sum_2 + sum_3;
 }
```

这个代码分为三部分，分别是求 sum_1、sum_2、sum_3。我们可以分别分析每一部分的时间复杂度，然后把它们放到一块儿，再取一个量级最大的作为整段代码的复杂度。

第一段的时间复杂度是多少呢？这段代码循环执行了 100 次，所以是一个常量的执行时间，跟 n 的规模无关。

即便这段代码循环 10000 次、100000 次，只要是一个已知的数，跟 n 无关，照样也是常量级的执行时间。当 n 无限大的时候，就可以忽略。尽管对代码的执行时间会有很大影响，但是回到时间复杂度的概念来说，它表示的是一个算法执行效率与数据规模增长的变化趋势，所以不管常量的执行时间多大，我们都可以忽略掉。因为它本身对增长趋势并没有影响。

那第二段代码和第三段代码的时间复杂度是多少呢？答案是 O(n) 和 $O(n^2)$。

综合这三段代码的时间复杂度，我们取其中最大的量级。所以，整段代码的时间复杂度就为 O(n2)。也就是说：**总的时间复杂度就等于量级最大的那段代码的时间复杂度**。那我们将这个规律抽象成公式就是：

如果 T1(n)=O(f(n))，T2(n)=O(g(n))；那么 T(n)=T1(n)+T2(n)=max(O(f(n)), O(g(n))) =O(max(f(n), g(n))).

**3. 乘法法则：嵌套代码的复杂度等于嵌套内外代码复杂度的乘积**

我刚讲了一个复杂度分析中的加法法则，这儿还有一个乘法法则。类比一下，你应该能“猜到”公式是什么样子的吧？

如果 $T1(n)=O(f(n))$，$T2(n)=O(g(n))$；那么 $T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n))$.

也就是说，假设 $T1(n) = O(n)$，$T2(n) = O(n2)$，则 $T1(n) * T2(n) = O(n3)$。落实到具体的代码上，我们可以把乘法法则看成是嵌套循环，我举个例子给你解释一下。

```c
int cal(int n) {
   int ret = 0;
   int i = 1;
   for (; i < n; ++i) {
     ret = ret + f(i);
   }
 }

 int f(int n) {
  int sum = 0;
  int i = 1;
  for (; i < n; ++i) {
    sum = sum + i;
  }
  return sum;
 }
```

我们单独看 cal() 函数。假设 f() 只是一个普通的操作，那第 4～6 行的时间复杂度就是，T1(n) = O(n)。但 f() 函数本身不是一个简单的操作，它的时间复杂度是 T2(n) = O(n)，所以，整个 cal() 函数的时间复杂度就是，$T(n)=T1(n)*T2(n)=O(n*n)=O(n2)$。

我刚刚讲了三种复杂度的分析技巧。不过，你并不用刻意去记忆。实际上，复杂度分析这个东西关键在于“熟练”。你只要多看案例，多分析，就能做到“无招胜有招”。

**几种常见时间复杂度实例分析**

复杂度量级（按数量级递增）

**多项式量级**

- 常量阶 O(1)
- 对数阶 O($\log n$)
- 线性阶 O(n)
- 线性对数阶 O($n\log n$)
- 平方阶 O($n^2$)、立方阶 O($n^3$)……K 次方阶 O($n^k$)

**非多项式量级（低效）**

- 指数阶 O($2^n$)
- 阶乘阶 O(n!)

把时间复杂度为非多项式量级的算法问题叫作 NP（Non-Deterministic Polynomial，非确定多项式）问题。

1. O(1)

```c
int i = 8;
int j = 6;
int sum = i + j;
```

一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是Ο(1)。

2. O($\log n$)、O($n\log n$)

```c
i=1;
while (i <= n)  {
  i = i * 2;
}
```

```c
i=1;
while (i <= n)  {
  i = i * 3;
}
```

在采用大 O 标记复杂度的时候，可以忽略系数，即 $O(Cf(n))$ = $O(f(n))$

3. $O(m+n)$、$O(m*n)$

代码的复杂度**由两个数据的规模**来决定

```c
int cal(int m, int n) {
  int sum_1 = 0;
  int i = 1;
  for (; i < m; ++i) {
    sum_1 = sum_1 + i;
  }

  int sum_2 = 0;
  int j = 1;
  for (; j < n; ++j) {
    sum_2 = sum_2 + j;
  }

  return sum_1 + sum_2;
}
```

从代码中可以看出，m 和 n 是表示两个数据规模。我们无法事先评估 m 和 n 谁的量级大，所以我们在表示复杂度的时候，就不能简单地利用加法法则，省略掉其中一个。所以，上面代码的时间复杂度就是 $O(m+n)$。

针对这种情况，原来的加法法则就不正确了，我们需要将加法规则改为：$T1(m) + T2(n) = O(f(m) + g(n))$。但是乘法法则继续有效：$T1(m)*T2(n)=O(f(m)*f(n))$。

### 空间复杂度分析

空间复杂度全称就是**渐进空间复杂度**（asymptotic space complexity），**表示算法的存储空间与数据规模之间的增长关系**。

```c
void print(int n) {
  int i = 0;
  int[] a = new int[n];
  for (i; i <n; ++i) {
    a[i] = i * i;
  }

  for (i = n-1; i >= 0; --i) {
    print out a[i]
  }
}
```

复杂度分析并不难，关键在于多练。

有人说，我们项目之前都会进行性能测试，再做代码的时间复杂度、空间复杂度分析，是不是多此一举呢？而且，每段代码都分析一下时间复杂度、空间复杂度，是不是很浪费时间呢？你怎么看待这个问题呢？

### 进一步复杂度分析

**最好情况时间复杂度**（best case time complexity）、**最坏情况时间复杂度**（worst case time complexity）、**平均情况时间复杂度**（average case time complexity）、**均摊时间复杂度**（amortized time complexity）

**最好、最坏情况时间复杂度**

```c
// n表示数组array的长度
int find(int[] array, int n, int x) {
  int i = 0;
  int pos = -1;
  for (; i < n; ++i) {
    if (array[i] == x) pos = i;
  }
  return pos;
}
```

这段代码要实现的功能是，在一个无序的数组（array）中，查找变量 x 出现的位置。如果没有找到，就返回 -1。按照上节课讲的分析方法，这段代码的复杂度是 O(n)，其中，n 代表数组的长度。

优化一下：

```c
// n表示数组array的长度
int find(int[] array, int n, int x) {
  int i = 0;
  int pos = -1;
  for (; i < n; ++i) {
    if (array[i] == x) {
       pos = i;
       break;
    }
  }
  return pos;
}
```

**平均情况时间复杂度**

**均摊时间复杂度**

```c
// array表示一个长度为n的数组
// 代码中的array.length就等于n
int[] array = new int[n];
int count = 0;

void insert(int val) {
  if (count == array.length) {
      int sum = 0;
      for (int i = 0; i < array.length; ++i) {
        sum = sum + array[i];
      }
      array[0] = sum;
      count = 1;
  }

  array[count] = val;
  ++count;
}
```

这段代码实现了一个往数组中插入数据的功能。当数组满了之后，也就是代码中的 count == array.length 时，我们用 for 循环遍历数组求和，并清空数组，将求和之后的 sum 值放到数组的第一个位置，然后再将新的数据插入。但如果数组一开始就有空闲空间，则直接将数据插入数组。

摊还分析法，通过摊还分析得到的时间复杂度我们起了一个名字，叫均摊时间复杂度。

![first-stage.jpeg](https://cdn.jsdelivr.net/gh/tianheg/static@wiki/images/ads/first-stage.jpeg)

## References

- [时间复杂度](https://zh.wikipedia.org/zh-cn/%E6%97%B6%E9%97%B4%E5%A4%8D%E6%9D%82%E5%BA%A6)
